{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the program is 0.004986286163330078\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import whois\n",
    "import urllib.request, re\n",
    "import xmltodict, json\n",
    "from datetime import datetime\n",
    "import ssl\n",
    "import OpenSSL\n",
    "import favicon\n",
    "import ipaddress\n",
    "from bs4 import BeautifulSoup\n",
    "import socket\n",
    "import requests\n",
    "from googlesearch import search\n",
    "from dateutil.parser import parse as date_parse\n",
    "import sys\n",
    "import dns.resolver\n",
    "from patterns import *\n",
    "import whois.parser\n",
    "start = time.time()\n",
    "\n",
    "def prepare_url(url):\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        response = requests.get(url,timeout=15)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(\"Oops!\", e.__class__, \"occurred .(soup error)\")    \n",
    "        response = \"\"\n",
    "        soup = -999\n",
    "    return soup\n",
    "\n",
    "\n",
    "def find_domain(url):\n",
    "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "    if re.match(r\"^.\",domain):\n",
    "        domain = domain.replace(\"www.\",\"\")\n",
    "    return domain\n",
    "\n",
    "\n",
    "\n",
    "def whois_response(domain):\n",
    "    try: \n",
    "        w = whois.whois(domain)\n",
    "        return w\n",
    "    except whois.parser.PywhoisError as e:\n",
    "        print(e.__class__, \"occurred\")\n",
    "        return -1\n",
    "    \n",
    "def exp_date(w):\n",
    "    if(w==-1):\n",
    "        return -1\n",
    "    \n",
    "    if type(w.expiration_date) == list:\n",
    "        expire_date = w.expiration_date[0]\n",
    "        return expire_date\n",
    "    else:\n",
    "        expire_date = w.expiration_date\n",
    "        return expire_date\n",
    "    \n",
    "def updatd_date(w):\n",
    "    if(w==-1):\n",
    "        return -1\n",
    "    if type(w.updated_date) == list:\n",
    "        up_date = w.updated_date[0]\n",
    "        return up_date\n",
    "    else:\n",
    "        up_date = w.updated_date\n",
    "        return up_date \n",
    "\n",
    "def create_date(w):\n",
    "    if(w==-1):\n",
    "        return -1\n",
    "    if type(w.creation_date) == list:\n",
    "        regis_date = w.creation_date[0]\n",
    "        return regis_date\n",
    "    else:\n",
    "        regis_date = w.creation_date\n",
    "        return regis_date\n",
    "\n",
    "\n",
    "def find_global_rank(url):\n",
    "    try:\n",
    "        xml = urllib.request.urlopen('http://data.alexa.com/data?cli=10&dat=s&url={}'.format(url)).read()\n",
    "        result= xmltodict.parse(xml)\n",
    "\n",
    "        data = json.dumps(result).replace(\"@\",\"\")\n",
    "        data_tojson = json.loads(data)\n",
    "        url = data_tojson[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"URL\"]\n",
    "        global_rank= int(data_tojson[\"ALEXA\"][\"SD\"][1][\"POPULARITY\"][\"TEXT\"])\n",
    "    except Exception as e:\n",
    "        print(\"Oops!\", e.__class__, \"occurred.\")    \n",
    "        global_rank=-1\n",
    "    \n",
    "    return global_rank\n",
    "\n",
    "\n",
    "def get_certificate(host, port=443, timeout=10):\n",
    "    try:\n",
    "        context = ssl.create_default_context()\n",
    "        conn = socket.create_connection((host, port))\n",
    "        sock = context.wrap_socket(conn, server_hostname=host)\n",
    "        sock.settimeout(timeout)\n",
    "        der_cert = sock.getpeercert(True)\n",
    "        #c = sock.getpeercert()\n",
    "        #print(c)\n",
    "        return ssl.DER_cert_to_PEM_cert(der_cert)\n",
    "\n",
    "    except:\n",
    "        print(\"SSL error\")\n",
    "        return -1\n",
    "\n",
    "    \n",
    "def domain_age(regis_date):\n",
    "    if(regis_date==-1):\n",
    "        return -1\n",
    "    \n",
    "    try:\n",
    "        today=datetime.now()\n",
    "        timedelta = today - regis_date\n",
    "        val = timedelta.days\n",
    "        print(\"Domain age:\",val)\n",
    "        if val <= 5000:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "#Features\n",
    "def having_ip_address(url):\n",
    "    try:\n",
    "        ipaddress.ip_address(url)\n",
    "        return -1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "def url_length(url):\n",
    "    if len(url) < 54:\n",
    "        return 1\n",
    "    if 54 <= len(url) <= 75:\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "\n",
    "\n",
    "def shortening_services(url):\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "def having_at_symbol(url):\n",
    "    if re.findall(\"@\", url):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "def double_slash_redirecting(url):\n",
    "    last_double_slash = url.rfind('//')\n",
    "    return -1 if last_double_slash > 6 else 1\n",
    "\n",
    "\n",
    "def prefix_suffix(url):\n",
    "    if re.findall(r\"https?://[^\\-]+-[^\\-]+/\", url):\n",
    "            return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "def having_sub_domain(url):\n",
    "    if having_ip_address(url) == -1:\n",
    "        match = re.search(\n",
    "            '(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.'\n",
    "            '([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5]))|(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',\n",
    "            url)\n",
    "        pos = match.end()\n",
    "        url = url[pos:]\n",
    "    num_dots = [x.start() for x in re.finditer(r'\\.', url)]\n",
    "    if len(num_dots) <= 3:\n",
    "        return 1\n",
    "    elif len(num_dots) == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "    \n",
    "    \n",
    "def ssl_final_state(certificate,dom_age):\n",
    "    try:\n",
    "        x509 = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, certificate)\n",
    "        extensions = (x509.get_extension(i) for i in range(x509.get_extension_count()))\n",
    "        extension_data = { e.get_short_name().decode('ascii'): str(e) for e in extensions}\n",
    "        #pprint(extension_data)\n",
    "\n",
    "        st=datetime.strptime(x509.get_notBefore().decode('utf-8'), '%Y%m%d%H%M%SZ'),\n",
    "        end=datetime.strptime(x509.get_notAfter().decode('utf-8'), '%Y%m%d%H%M%SZ'),\n",
    "\n",
    "        e=str(*st)\n",
    "        f=str(*end)\n",
    "\n",
    "        start_date = datetime.strptime(e,'%Y-%m-%d %H:%M:%S')\n",
    "        print(\"\\nssl start:\",start_date)\n",
    "        exp_date = datetime.strptime(f,'%Y-%m-%d %H:%M:%S')\n",
    "        print(\"ssl end:\",exp_date)\n",
    "\n",
    "        timedelta = exp_date - start_date\n",
    "        val = timedelta.days\n",
    "        #print(\"ssl will expire in \" +str(val) +\" days\")\n",
    "\n",
    "        \n",
    "        c=extension_data['certificatePolicies']\n",
    "        li=['2.23.140.1.2.1','2.23.140.1.2.2','2.23.140.1.1',]\n",
    "    \n",
    "        for i in li:\n",
    "            if (c.find(i) != -1):\n",
    "                policy=i\n",
    "                \n",
    "\n",
    "        if policy==li[0] or certificate==-1: #domain_v\n",
    "            print(\"policy is DV\",policy)\n",
    "            if dom_age==1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    \n",
    "        \n",
    "        if policy==li[1]:   #organization_v\n",
    "            print(\"policy is OV\",policy)\n",
    "            if dom_age==1:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "            \n",
    "        if policy==li[2]:  #extended_v\n",
    "            print(\"policy is EV\",policy)\n",
    "            return 1\n",
    "    \n",
    "    except:\n",
    "        #print(\"error\")\n",
    "        return -1\n",
    "\n",
    "\n",
    "    \n",
    "def domain_reg_length(expire_date):\n",
    "    if(expire_date==-1):\n",
    "        return -1\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        today=datetime.now()\n",
    "        timedelta = expire_date - today\n",
    "        val = timedelta.days\n",
    "        print(\"days to expire: \" +str(val))\n",
    "                       \n",
    "        if val / 365 <=1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "        \n",
    "\n",
    "def fav_icon(url):\n",
    "    try:\n",
    "        fav=0\n",
    "        icons=favicon.get(url)\n",
    "        #print(icons)   \n",
    "        #icon=icons[0]\n",
    "        #print(icon.url)\n",
    "        for i in icons:\n",
    "            if i.format =='ico':\n",
    "                fav+=1\n",
    "                print(\"has favicon\")\n",
    "                #print(i.url)\n",
    "                return 1\n",
    "    \n",
    "        if(fav==0):\n",
    "            print(\"no favicon\")\n",
    "            return -1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "        \n",
    "\n",
    "def port(domain):\n",
    "    try:\n",
    "        port = domain.split(\":\")[1]\n",
    "        if port:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "\n",
    "\n",
    "def https_token(url):\n",
    "    if re.findall(r\"^https://\", url):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def request_url(url, soup, domain):\n",
    "    i = 0\n",
    "    success = 0\n",
    "    if soup==-999:\n",
    "        return -1\n",
    "    else:\n",
    "        for img in soup.find_all('img', src=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', img['src'])]\n",
    "            if url in img['src'] or domain in img['src'] or len(dots) == 1:\n",
    "                success = success + 1\n",
    "            i = i + 1\n",
    "\n",
    "        for audio in soup.find_all('audio', src=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', audio['src'])]\n",
    "            if url in audio['src'] or domain in audio['src'] or len(dots) == 1:\n",
    "                success = success + 1\n",
    "            i = i + 1\n",
    "\n",
    "        for embed in soup.find_all('embed', src=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', embed['src'])]\n",
    "            if url in embed['src'] or domain in embed['src'] or len(dots) == 1:\n",
    "                success = success + 1\n",
    "            i = i + 1\n",
    "\n",
    "        for i_frame in soup.find_all('i_frame', src=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', i_frame['src'])]\n",
    "            if url in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1:\n",
    "                success = success + 1\n",
    "            i = i + 1\n",
    "\n",
    "        try:\n",
    "            percentage = success / float(i) * 100\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "        if percentage < 22.0:\n",
    "            return 1\n",
    "        elif 22.0 <= percentage < 61.0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "\n",
    "def url_of_anchor(url,soup,domain):\n",
    "    percentage = 0\n",
    "    i = 0\n",
    "    unsafe=0\n",
    "    if soup == -999:\n",
    "        return -1\n",
    "    else:\n",
    "        for a in soup.find_all('a', href=True):\n",
    "        \n",
    "            if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (url in a['href'] or domain in a['href']):\n",
    "                unsafe = unsafe + 1\n",
    "            i = i + 1\n",
    "        try:\n",
    "            percentage = unsafe / float(i) * 100\n",
    "        except:\n",
    "            return 1\n",
    "        if percentage < 31.0:\n",
    "            return 1\n",
    "        elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "\n",
    "def links_in_tags(url, soup, domain):\n",
    "    i=0\n",
    "    success =0\n",
    "    if soup == -999:\n",
    "        return -1\n",
    "    else:\n",
    "        for link in soup.find_all('link', href=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', link['href'])]\n",
    "            if url in link['href'] or domain in link['href'] or len(dots) == 1:\n",
    "                success = success + 1\n",
    "            i = i + 1\n",
    "\n",
    "        for script in soup.find_all('script', src=True):\n",
    "            dots = [x.start() for x in re.finditer(r'\\.', script['src'])]\n",
    "            if url in script['src'] or domain in script['src'] or len(dots) == 1:\n",
    "                success = success + 1\n",
    "            i = i + 1\n",
    "        try:\n",
    "            percentage = success / float(i) * 100\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "        if percentage < 17.0:\n",
    "            return 1\n",
    "        elif 17.0 <= percentage < 81.0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "\n",
    "def sfh(url,soup,domain):\n",
    "    if soup==-999:\n",
    "        return -1\n",
    "    else:\n",
    "        for form in soup.find_all('form', action=True):\n",
    "            if form['action'] == \"\" or form['action'] == \"about:blank\":\n",
    "                return -1\n",
    "            elif url not in form['action'] and domain not in form['action']:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def submitting_to_email(soup):\n",
    "    if soup==-999:\n",
    "        return -1\n",
    "    else:\n",
    "        for form in soup.find_all('form', action=True):\n",
    "            if \"mailto:\" in form['action']:\n",
    "                return -1 \n",
    "            else:\n",
    "                return 1\n",
    "    return 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def abnormal_url(domain, who): \n",
    "    if(who==-1):\n",
    "        return -1\n",
    "    try:\n",
    "        print(who.domain_name)\n",
    "        if type(who.domain_name) == list:\n",
    "            w_dom=who.domain_name[0]\n",
    "        \n",
    "        else:\n",
    "            w_dom=who.domain_name\n",
    "    \n",
    "        print(\"\\nURL Domain \",domain.lower())\n",
    "        print(\"Whois_domain \",w_dom.lower())\n",
    "        if domain.lower()==w_dom.lower():\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def redirect(url):\n",
    "    try:\n",
    "        response=requests.get(url)\n",
    "        if response == \"\":\n",
    "            return -1\n",
    "        else:\n",
    "            #print(\"response length\",len(response.history))\n",
    "            if len(response.history) <= 1:\n",
    "                return -1\n",
    "            elif len(response.history) <= 4:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def popup_window(url):\n",
    "    try:\n",
    "        response=requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        if response == \"\":\n",
    "            return -1\n",
    "        else:\n",
    "            if re.findall(r\"prompt\\(\", response.text):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def age_of_domain(expire_date,regis_date):\n",
    "    if(expire_date==-1):\n",
    "        return -1\n",
    "    \n",
    "    try:\n",
    "        timedelta = expire_date - regis_date\n",
    "        ageofdomain = timedelta.days\n",
    "        #print(\"Age of domain in days: \",ageofdomain)\n",
    "        \n",
    "        if ageofdomain <= 365:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "\n",
    "def dns_record(domain,w):\n",
    "    try:\n",
    "        my_resolver = dns.resolver.Resolver()\n",
    "        # Finding A record \n",
    "        result = my_resolver.resolve(domain, 'A') \n",
    "        # Printing record \n",
    "        for val in result: \n",
    "            #print('\\nA Record : ',val)\n",
    "            c=str(val)\n",
    "    \n",
    "            is_valid = re.match(\"^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$\",c)\n",
    "            if is_valid:\n",
    "                cond=True\n",
    "    \n",
    "        if w!=-1 and cond==True:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        rank= int(rank)\n",
    "        if (rank<100000):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except TypeError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def links_pointing_to_page(url,soup):\n",
    "    try:    \n",
    "        externalLinks = []\n",
    "        for link in soup.find_all('a', {'href' : re.compile('^(http|www)((?!'+url+').)*$')}):\n",
    "            if link.attrs['href'] is not None:\n",
    "                if link.attrs['href'] not in externalLinks:\n",
    "                    externalLinks.append(link.attrs['href'])\n",
    "        \n",
    "        number_of_links =len(externalLinks)\n",
    "        print(\"no. of links: \",number_of_links)\n",
    "        if number_of_links == 0:\n",
    "            return -1\n",
    "        elif number_of_links <=5:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def generate_dataset(url):\n",
    "    url=prepare_url(url)\n",
    "    soup=get_soup(url)\n",
    "    \n",
    "    domain=find_domain(url)\n",
    "    \n",
    "    who_resp=whois_response(domain)\n",
    "    #print(\"who_resp:\",who_resp)\n",
    "    \n",
    "    global_rank=find_global_rank(url)\n",
    "    expire_date=exp_date(who_resp)\n",
    "    #print(\"Expiration date \"+ str(expire_date))\n",
    "    \n",
    "    regis_date=create_date(who_resp)\n",
    "    #print(\"Registration date \"+ str(regis_date))\n",
    "\n",
    "    updat_date=updatd_date(who_resp)\n",
    "    #print(\"Updated date \"+ str(updat_date))\n",
    "    \n",
    "    dom_age=domain_age(regis_date)\n",
    "    \n",
    "    certificate = get_certificate(domain)\n",
    "    ss_l=ssl_final_state(certificate,dom_age)\n",
    "    \n",
    "    data_set=[]\n",
    "    data_set.append(having_ip_address(url))          #1\n",
    "    data_set.append(url_length(url))\n",
    "    data_set.append(shortening_services(url))\n",
    "    data_set.append(having_at_symbol(url))\n",
    "    data_set.append(double_slash_redirecting(url))\n",
    "    data_set.append(prefix_suffix(url))\n",
    "    data_set.append(having_sub_domain(url))\n",
    "    \n",
    "    data_set.append(ss_l)     #8\n",
    "    data_set.append(domain_reg_length(expire_date))\n",
    "    data_set.append(fav_icon(url))            #10\n",
    "    data_set.append(port(domain))\n",
    "    data_set.append(https_token(url))\n",
    "    data_set.append(request_url(url,soup,domain))        #13\n",
    "    data_set.append(url_of_anchor(url,soup,domain))\n",
    "    data_set.append(sfh(url,soup,domain))\n",
    "    data_set.append(submitting_to_email(soup))           #16\n",
    "    data_set.append(abnormal_url(domain,who_resp))\n",
    "    data_set.append(redirect(url))\n",
    "    data_set.append(popup_window(url))                   #19\n",
    "    data_set.append(age_of_domain(expire_date,regis_date))\n",
    "    data_set.append(dns_record(domain,who_resp))                #21\n",
    "    data_set.append(web_traffic(url))\n",
    "    data_set.append(links_pointing_to_page(url,soup))     #23\n",
    "    return data_set,global_rank,ss_l\n",
    "\n",
    "#url=\"https://www.axisbank.com/\"\n",
    "#url=\"https://www.hdfcbank.com/\"\n",
    "#url=\"https://www.onlinesbi.com/\"\n",
    "#url=\"https://www.ssl.com/article/dv-ov-and-ev-certificates/\"    \n",
    "#url=\"https://www.amazon.in/\"\n",
    "#url=\"https://www.paypal.com/in/signin\"\n",
    "#url=\"https://www.airtel.in/bank/\"\n",
    "#url=\"https://www.jiopaymentsbank.com/\"\n",
    "#url=\"https://www.amazon.in/amazonpay/home?ref_=apay_logo_APayDashboard\"\n",
    "\n",
    "#c,r,s=generate_dataset(url)\n",
    "#print(\"RANK\",r)\n",
    "#print(c) \n",
    "end = time.time()\n",
    "# total time taken\n",
    "print(\"Runtime of the program is\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
