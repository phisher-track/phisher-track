{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime of the program is 0.0019941329956054688\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from tldextract import extract\n",
    "from mechanize import Browser \n",
    "import requests\n",
    "import string\n",
    "\n",
    "\n",
    "def quer_to_be_searched(r,high,dom,url):\n",
    "    \n",
    "    try:\n",
    "       \n",
    "        #CODE TO EXTRACT TITLE        \n",
    "        br = Browser() \n",
    "        br.set_handle_robots(False)\n",
    "        br.open(url) \n",
    "        n_title= br.title()        \n",
    "        \n",
    "    except:\n",
    "        #TO GENERATE QUERY WITHOUT TITLE OF WEBSITE\n",
    "        #print(\"No title found.....The website has no title\")\n",
    "        search=\"\"\n",
    "        for i in high:\n",
    "            search += i[0]+ ' ' \n",
    "        \n",
    "        z =(\" \".join([dom, search]))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        #TO GENERATE QUERY WITH TITLE\n",
    "        search=\"\"\n",
    "        for i in high:\n",
    "            search += i[0]+ ' '\n",
    "        \n",
    "            \n",
    "        z =(\" \".join([n_title,dom, search]))\n",
    "   \n",
    "    return z\n",
    "    \n",
    "    \n",
    "    \n",
    "#TO COMPARE THE SITES    \n",
    "def site_check(webs,query):\n",
    "    c=0\n",
    "    count=1\n",
    "    \n",
    "    while(c<=9):\n",
    "                    \n",
    "        try:\n",
    "            url=webs[c]      \n",
    "            r = requests.get(url,timeout=5)\n",
    "\n",
    "        except:\n",
    "            #print(\"Enough sites were not found....\")\n",
    "            c=10\n",
    "        \n",
    "        else:\n",
    "            website_content = BeautifulSoup(r.text, 'html.parser').get_text()\n",
    "            website_content = website_content.replace(\"\\n\",\" \")\n",
    "            result = calculate_frequency(website_content)\n",
    "        \n",
    "            k = Counter(result) \n",
    "            high = k.most_common(3) \n",
    "    \n",
    "            info= extract(url)\n",
    "            dom = info.domain\n",
    "            \n",
    "            \n",
    "            a=quer_to_be_searched(r,high,dom,url)\n",
    "            print(\"Query:\" + str(count) + \" \" +a)\n",
    "    \n",
    "            count+=1\n",
    "            c+=1\n",
    "            \n",
    "            if query==a:\n",
    "                #legitimate\n",
    "                c=15\n",
    "                \n",
    "        \n",
    "    if c==15:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1                          \n",
    "    \n",
    "    \n",
    "    \n",
    "def calculate_frequency(content):\n",
    "    \n",
    "    stopwords= ['a', 'about', 'above', 'after', 'again', 'against', 'all',\n",
    "                'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', \n",
    "                'because', 'been', 'before', 'being', 'below', 'between', \n",
    "                'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", \n",
    "                'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', \n",
    "                'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", \n",
    "                'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\",\n",
    "                \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', \n",
    "                'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', \n",
    "                'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', \n",
    "                'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', \n",
    "                'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', \n",
    "                'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', \n",
    "                'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', \n",
    "                'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \n",
    "                \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', \n",
    "                'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", \n",
    "                'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", \n",
    "                'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \n",
    "                \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "     \n",
    "    \n",
    "        \n",
    "    new_content = \"\"\n",
    "    for char in content:\n",
    "        if char in string.punctuation:\n",
    "            continue\n",
    "        new_content+=char.lower()\n",
    "    result = {}     \n",
    "\n",
    "    content_list = new_content.split(\" \")\n",
    "    content_list = [element for element in content_list if element != \"\"]\n",
    "    for word in content_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if word in result.keys():\n",
    "            result[word]+=1\n",
    "            \n",
    "        else:\n",
    "            result[word]=1\n",
    "            \n",
    "    result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1],reverse = True)} \n",
    "    return result\n",
    "  \n",
    "\n",
    "#url=\"https://www.axisbank.com/\"\n",
    "#url=\"https://www.onlinesbi.com/\"\n",
    "#url=\"https://www.ssl.com/article/dv-ov-and-ev-certificates/\"    \n",
    "#url=\"https://www.amazon.in/\"\n",
    "#url=\"https://www.paypal.com/in/signin\"\n",
    "#url=\"https://www.airtel.in/bank/\"\n",
    "#url=\"https://www.jiopaymentsbank.com/\"\n",
    "\n",
    "    \n",
    "def content(url):\n",
    "    #f = open(\"leg.txt\", \"r\")\n",
    "    #c=f.read()\n",
    "    #if url in c:\n",
    "     #   return 1\n",
    "    \n",
    "    result = dict()\n",
    "    try:\n",
    "        r = requests.get(url,timeout=30)\n",
    "\n",
    "        website_content = BeautifulSoup(r.text, 'html.parser').get_text()\n",
    "        website_content = website_content.replace(\"\\n\",\" \")\n",
    "        result = calculate_frequency(website_content)\n",
    "        new_result = [item for item in result if not item.isdigit()]\n",
    "    \n",
    "        #CODE TO GET THE MOST COMMON WORDS\n",
    "        k = Counter(new_result) \n",
    "        #print(k)\n",
    "        high = k.most_common(3) \n",
    "    \n",
    "    \n",
    "        #CODE TO EXTRACT DOMAIN NAME\n",
    "        info= extract(url)\n",
    "        dom = info.domain\n",
    "\n",
    "        z1=quer_to_be_searched(r,high,dom,url)\n",
    "        print(\"Query: \" + z1)\n",
    "        \n",
    "    \n",
    "        #CODE TO GET THE SITES FROM GOOGLE SEARCH ENGINE\n",
    "        try:\n",
    "            from googlesearch import search \n",
    "        except ImportError:\n",
    "            print(\"No module named 'google' found\")\n",
    "            return -1\n",
    "  \n",
    "        query = z1\n",
    "        abc=\"\"\n",
    "        who=[]\n",
    "        for j in search(query, tld=\"com\", num=10,stop=10, pause=2):\n",
    "            abc=j\n",
    "            print(abc)\n",
    "            who.append(abc)\n",
    "        \n",
    "        c1=site_check(who,query)\n",
    "        return c1    \n",
    "    \n",
    "    except :\n",
    "        return -1\n",
    "\n",
    "#c=content(url)\n",
    "#print(c)\n",
    "\n",
    "end = time.time()\n",
    "# total time taken\n",
    "print(\"Runtime of the program is\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
